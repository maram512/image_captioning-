# -*- coding: utf-8 -*-
"""Deep_Learning_Project (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lqaK1S_AeVPPdGZ2XKX-VbBffzla5THE
"""

pip install pycocotools tqdm requests

# === Cell 1: Install dependencies ===
!pip install -q torch torchvision torchaudio transformers pycocotools nltk tqdm pillow

import math, random
from PIL import Image
from tqdm import tqdm
from collections import Counter

import nltk
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

from transformers import ViTModel, AutoFeatureExtractor
nltk.download("punkt")

from pycocotools.coco import COCO
import requests
import os
import json
from tqdm import tqdm
import zipfile

# Number of images you want
NUM_IMAGES = 5000   # you can change it

# COCO annotation file (download only 50MB instead of 25GB images)
annotation_zip_url = "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
annotation_zip_filename = "annotations_trainval2017.zip"
extracted_annotation_folder = "annotations"
ann_file = os.path.join(extracted_annotation_folder, "captions_train2017.json")

# Download and extract annotations if they don't exist
if not os.path.exists(ann_file):
    print(f"Downloading {annotation_zip_filename}...")
    response = requests.get(annotation_zip_url, stream=True)
    response.raise_for_status() # Ensure we got a successful response

    with open(annotation_zip_filename, "wb") as f:
        for chunk in tqdm(response.iter_content(chunk_size=8192), desc="Downloading annotations"): # Added tqdm for download progress
            f.write(chunk)
    print(f"Extracting {annotation_zip_filename}...")
    with zipfile.ZipFile(annotation_zip_filename, 'r') as zip_ref:
        zip_ref.extractall(".") # Extract to the current directory
    print("Extraction complete.")
else:
    print(f"{ann_file} already exists. Skipping download and extraction.")

# Output folders
save_images = "mini_coco/images"
save_annotations = "mini_coco/annotations.json"
os.makedirs(save_images, exist_ok=True)

# Load COCO
coco = COCO(ann_file)

# Get all image IDs
all_ids = coco.getImgIds()
subset_ids = all_ids[:NUM_IMAGES]

mini_anns = []
mini_images = [] # New list to store image metadata

for img_id in tqdm(subset_ids, desc="Processing images"):
    # Load image info
    info = coco.loadImgs(img_id)[0]
    mini_images.append(info) # Collect image metadata for the subset
    url = info['coco_url']

    # Download image
    filepath = os.path.join(save_images, info['file_name'])
    # Check if the image already exists before downloading
    if not os.path.exists(filepath):
        try:
            img_data = requests.get(url, timeout=10).content # Added timeout
            with open(filepath, 'wb') as f:
                f.write(img_data)
        except requests.exceptions.RequestException as e:
            print(f"Could not download image {url}: {e}")
            continue

    # Get caption annotations
    ann_ids = coco.getAnnIds(imgIds=img_id)
    anns = coco.loadAnns(ann_ids)
    mini_anns.extend(anns)

# Save annotations and image metadata in the expected COCO format
with open(save_annotations, "w") as f:
    json.dump({"annotations": mini_anns, "images": mini_images}, f)

print(" Mini COCO dataset created successfully!")

import shutil

zip_path = "mini_coco_images.zip"
folder_path = "mini_coco/images"

# If zip does not exist, create it once
if not os.path.exists(zip_path):
    print("Zipping processed images...")
    shutil.make_archive("mini_coco_images", 'zip', folder_path)
    print("Zip created successfully!")

# Download the zip file
from google.colab import files
files.download(zip_path)

# === Cell 4: Simple tokenizer & Vocabulary ===
# We'll build a simple word-level tokenizer from captions.

class Vocabulary:
    def __init__(self, freq_threshold=5, max_size=None):
        self.freq_threshold = freq_threshold
        self.max_size = max_size
        self.pad_token = "<pad>"
        self.start_token = "<start>"
        self.end_token = "<end>"
        self.unk_token = "<unk>"
        self.word2idx = {}
        self.idx2word = {}
        self._init_specials()

    def _init_specials(self):
        specials = [self.pad_token, self.start_token, self.end_token, self.unk_token]
        for i, s in enumerate(specials):
            self.word2idx[s] = i
            self.idx2word[i] = s
        self.next_index = len(specials)

    def build_vocab(self, sentence_list):
        counter = Counter()
        for sent in sentence_list:
            tokens = self.tokenize(sent)
            counter.update(tokens)
        # keep tokens above threshold
        items = [w for w,c in counter.items() if c >= self.freq_threshold]
        items.sort(key=lambda w: (-counter[w], w))
        if self.max_size:
            items = items[:self.max_size]
        for w in items:
            if w not in self.word2idx:
                idx = self.next_index
                self.word2idx[w] = idx
                self.idx2word[idx] = w
                self.next_index += 1

    def __len__(self):
        return len(self.word2idx)

    def tokenize(self, text):
        # Basic preprocessing: lowercase + split on whitespace + nltk word_tokenize
        text = text.lower().strip()
        tokens = nltk.word_tokenize(text)
        return tokens

    def numericalize(self, text):
        tokens = [self.start_token] + self.tokenize(text) + [self.end_token]
        indices = [ self.word2idx.get(t, self.word2idx[self.unk_token]) for t in tokens ]
        return indices

# === Cell 5: Dataset and DataLoader ===

class CocoCaptionDataset(Dataset):
    def __init__(self, images_dir, ann_file, vocab: Vocabulary, transform=None, max_len=30):
        # ann_file: json saved by download_coco_subset() or original COCO captions
        with open(ann_file, 'r', encoding='utf8') as f:
            data = json.load(f)
        # handle different json shapes
        images_meta = data.get("images", data.get("images", []))
        annotations = data.get("annotations", data.get("annotations", []))
        # Build map image_id -> filename
        self.id2file = {}
        for img in images_meta:
            self.id2file[img['id']] = img['file_name']
        # Build list of (image_path, caption)
        self.samples = []
        for ann in annotations:
            img_id = ann['image_id']
            if img_id in self.id2file:
                img_file = os.path.join(images_dir, self.id2file[img_id])
                cap = ann['caption']
                self.samples.append((img_file, cap))
        self.vocab = vocab
        self.transform = transform
        self.max_len = max_len

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, caption = self.samples[idx]
        # handle missing image
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            # return a black image if missing
            image = Image.new('RGB', (224,224), (0,0,0))
        if self.transform:
            image = self.transform(image)
        # numericalize caption
        cap_indices = self.vocab.numericalize(caption)
        if len(cap_indices) > self.max_len:
            cap_indices = cap_indices[:self.max_len-1] + [self.vocab.word2idx[self.vocab.end_token]]
        return image, torch.tensor(cap_indices, dtype=torch.long)

# Collate function to pad sequences
def collate_fn(batch):
    images, captions = zip(*batch)
    images = torch.stack(images)
    lengths = [len(c) for c in captions]
    max_len = max(lengths)
    pad_idx = vocab.word2idx[vocab.pad_token]
    caps_padded = torch.full((len(captions), max_len), pad_idx, dtype=torch.long)
    for i, c in enumerate(captions):
        caps_padded[i, :len(c)] = c
    return images, caps_padded, torch.tensor(lengths, dtype=torch.long)

nltk.download('punkt_tab')
# === Cell 6: Build vocab from subset captions (run after download) ===

# Adjust these paths if different.
images_dir = "/content/mini_coco/images"
subset_ann = "/content/mini_coco/annotations.json"

# Read captions and build vocab
with open(subset_ann, 'r', encoding='utf8') as f:
    data = json.load(f)
captions = [ann['caption'] for ann in data['annotations']]

# Create vocab
vocab = Vocabulary(freq_threshold=3, max_size=10000)  # tune thresholds
vocab.build_vocab(captions)
print("Vocab size:", len(vocab))

# Transforms for ViT: use AutoFeatureExtractor to match ViT preprocessing
feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
])

# === Cell 7: Create Dataset and DataLoader ===
dataset = CocoCaptionDataset(images_dir, subset_ann, vocab, transform=transform, max_len=30)

# Split to train/val
val_frac = 0.2
n = len(dataset)
n_val = max(1, int(n * val_frac))
n_train = n - n_val
train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])

batch_size = 32
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2)

print("Train samples:", len(train_set), "Val samples:", len(val_set))

# === Cell 8: Model definition ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        pe = pe.unsqueeze(1) # (max_len, 1, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (seq_len, batch, d_model)
        seq_len = x.size(0)
        x = x + self.pe[:seq_len]
        return x

class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size, decoder_dim=512, nhead=8, num_layers=4, vit_model_name="google/vit-base-patch16-224-in21k", dropout=0.1):
        super().__init__()
        # ViT encoder
        self.vit = ViTModel.from_pretrained(vit_model_name)
        vit_hidden = self.vit.config.hidden_size  # usually 768
        self.enc_proj = nn.Linear(vit_hidden, decoder_dim)  # project each patch embedding to decoder dim

        # token embedding for decoder
        self.token_emb = nn.Embedding(vocab_size, decoder_dim)
        self.pos_enc = PositionalEncoding(decoder_dim, max_len=100)

        # Transformer decoder
        decoder_layer = nn.TransformerDecoderLayer(d_model=decoder_dim, nhead=nhead, dim_feedforward=decoder_dim*4, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        # final projection to vocab
        self.generator = nn.Linear(decoder_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)
        self.decoder_dim = decoder_dim

    def forward(self, images, captions, caption_lengths=None):
        # images: (batch, 3, H, W)
        batch_size = images.size(0)
        # ViT outputs: last_hidden_state shape (batch, num_patches+1, hidden)
        vit_outputs = self.vit(pixel_values=images)
        enc = vit_outputs.last_hidden_state  # (batch, seq_len, vit_hidden)
        enc = self.enc_proj(enc)  # (batch, seq_len, decoder_dim)

        # Prepare target embeddings
        # captions: (batch, tgt_len)
        tgt_emb = self.token_emb(captions)  # (batch, tgt_len, decoder_dim)
        # Transformer in PyTorch expects (tgt_seq, batch, dim) and memory (src_seq, batch, dim)
        memory = enc.permute(1,0,2)  # (src_seq, batch, d)
        tgt = tgt_emb.permute(1,0,2)  # (tgt_seq, batch, d)
        tgt = self.pos_enc(tgt)

        # Create masks
        tgt_seq_len = tgt.size(0)
        # subsequent mask for causal decoding
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(images.device)

        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (tgt_seq, batch, d)
        output = output.permute(1,0,2)  # (batch, tgt_seq, d)
        logits = self.generator(output)  # (batch, tgt_seq, vocab)
        return logits

    def encode_image(self, images):
        vit_outputs = self.vit(pixel_values=images)
        enc = vit_outputs.last_hidden_state
        enc = self.enc_proj(enc)
        return enc  # (batch, src_seq, d)

    def decode_greedy(self, images, max_len=30, start_idx=1, end_idx=2):
        # start_idx and end_idx refer to vocab indices (depends on vocab)
        self.eval()
        with torch.no_grad():
            memory = self.encode_image(images)  # (batch, src_seq, d)
            memory = memory.permute(1,0,2)  # (src_seq, batch, d)
            batch = images.size(0)
            generated = torch.full((batch, 1), start_idx, dtype=torch.long, device=images.device)  # (batch, 1)

            for t in range(max_len-1):
                tgt_emb = self.token_emb(generated)  # (batch, seq, d)
                tgt = tgt_emb.permute(1,0,2)
                tgt = self.pos_enc(tgt)
                tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(images.device)
                out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (tgt_seq, batch, d)
                out_last = out[-1]  # (batch, d)
                logits = self.generator(out_last)  # (batch, vocab)
                next_word = logits.argmax(dim=-1).unsqueeze(1)  # (batch,1)
                generated = torch.cat([generated, next_word], dim=1)
            # convert to list
            results = []
            for i in range(batch):
                seq = generated[i].tolist()
                # trim at end_idx
                if end_idx in seq:
                    seq = seq[:seq.index(end_idx)+1]
                results.append(seq)
            return results

from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

# === Cell 9: Training loop ===
def train_one_epoch(model, dataloader, optimizer, criterion, epoch, clip=1.0, grad_accum_steps=1):
    model.train()
    total_loss = 0.0
    for i, (images, captions, lengths) in enumerate(tqdm(dataloader, desc=f"Train Epoch {epoch}")):
        images = images.to(device)
        captions = captions.to(device)  # (batch, tgt_len)
        # prepare inputs and targets: decoder input is captions[:, :-1], target is captions[:, 1:]
        inputs = captions[:, :-1]
        targets = captions[:, 1:]
        optimizer.zero_grad()
        logits = model(images, inputs)
        # logits: (batch, seq_len, vocab)
        vocab_size = logits.size(-1)
        logits_flat = logits.reshape(-1, vocab_size)
        targets_flat = targets.reshape(-1)
        loss = criterion(logits_flat, targets_flat)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0.0
    all_references = []
    all_hypotheses = []
    with torch.no_grad():
        for images, captions, lengths in tqdm(dataloader, desc="Eval"):
            images = images.to(device)
            captions = captions.to(device)
            inputs = captions[:, :-1]
            targets = captions[:, 1:]
            logits = model(images, inputs)
            vocab_size = logits.size(-1)
            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))
            total_loss += loss.item()
            # decode greedy for BLEU (convert ids to words)
            hypo_ids = model.decode_greedy(images, max_len=30, start_idx=vocab.word2idx[vocab.start_token], end_idx=vocab.word2idx[vocab.end_token])
            for i, hid in enumerate(hypo_ids):
                # convert to words and remove start & end tokens
                words = [vocab.idx2word.get(idx, vocab.unk_token) for idx in hid]
                # remove start token if present
                if words and words[0] == vocab.start_token:
                    words = words[1:]
                # remove end token and anything after
                if vocab.end_token in words:
                    words = words[:words.index(vocab.end_token)]
                all_hypotheses.append(words)
                # reference: original caption (we'll tokenize)
                ref_seq = captions[i].cpu().tolist()
                # convert to words
                ref_words = [vocab.idx2word.get(idx, vocab.unk_token) for idx in ref_seq]
                # strip start and pad until end token
                if ref_words and ref_words[0] == vocab.start_token:
                    ref_words = ref_words[1:]
                if vocab.end_token in ref_words:
                    ref_words = ref_words[:ref_words.index(vocab.end_token)]
                all_references.append([ref_words])  # list of references for corpus_bleu
    # compute BLEU
    smoothie = SmoothingFunction().method4
    bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1,0,0,0), smoothing_function=smoothie)
    bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothie)
    return total_loss / len(dataloader), bleu1, bleu4

# === Cell 10: Initialize model, optimizer, criterion ===
vocab_size = len(vocab)
model = ImageCaptioningModel(vocab_size=vocab_size, decoder_dim=512, nhead=8, num_layers=4, vit_model_name="google/vit-base-patch16-224-in21k").to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)
criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])

print("Model ready. Parameters:", sum(p.numel() for p in model.parameters() if p.requires_grad))

# === Cell: Training loop with metrics tracking + saving ===

from google.colab import files
import shutil

num_epochs = 10
best_bleu4 = 0.0
save_dir = "checkpoints"
os.makedirs(save_dir, exist_ok=True)

# lists to track metrics across epochs
train_losses = []
val_losses = []
bleu1_scores = []
bleu4_scores = []

for epoch in range(1, num_epochs + 1):

    print(f"\n Epoch {epoch}/{num_epochs}")

    # Train
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, epoch)

    # Evaluate
    val_loss, bleu1, bleu4 = evaluate(model, val_loader, criterion)

    # Print epoch summary
    print(f"Epoch {epoch}:")
    print(f"  ðŸ”¹ Train Loss: {train_loss:.4f}")
    print(f"  ðŸ”¹ Val Loss:   {val_loss:.4f}")
    print(f"  ðŸ”¹ BLEU-1:     {bleu1:.4f}")
    print(f"  ðŸ”¹ BLEU-4:     {bleu4:.4f}")

    # Store metrics
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    bleu1_scores.append(bleu1)
    bleu4_scores.append(bleu4)

    # Prepare checkpoint
    ckpt = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "vocab": vocab.word2idx
    }

    # Save epoch checkpoint
    epoch_path = os.path.join(save_dir, f"caption_model_epoch{epoch}.pt")
    torch.save(ckpt, epoch_path)
    print(f"   Saved: {epoch_path}")

    # Save best model
    if bleu4 > best_bleu4:
        best_bleu4 = bleu4
        best_path = os.path.join(save_dir, "best_caption_model.pt")
        torch.save(ckpt, best_path)
        print(f"   New BEST model saved: {best_path}")

print("\n Training complete!")

import matplotlib.pyplot as plt

epochs = range(1, num_epochs + 1)

plt.figure(figsize=(14,5))

# Loss curves
plt.subplot(1,2,1)
plt.plot(epochs, train_losses, label='Train Loss')
plt.plot(epochs, val_losses, label='Validation Loss')
plt.title("Train vs Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.legend()

# BLEU curves
plt.subplot(1,2,2)
plt.plot(epochs, bleu1_scores, label='BLEU-1')
plt.plot(epochs, bleu4_scores, label='BLEU-4')
plt.title("BLEU Scores Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("BLEU Score")
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# Zip checkpoints for download
zip_filename = "checkpoints.zip"
shutil.make_archive("checkpoints", "zip", save_dir)

print(f" Zipped checkpoint folder: {zip_filename}")

# === Cell 12: Inference helper (convert ids -> sentence) ===
def ids_to_sentence(id_list, vocab):
    words = [vocab.idx2word.get(i, vocab.unk_token) for i in id_list]
    # remove <start>
    if words and words[0] == vocab.start_token:
        words = words[1:]
    # truncate at <end>
    if vocab.end_token in words:
        words = words[:words.index(vocab.end_token)]
    return " ".join(words)

# Example: load best model and run on a few val images
ckpt = torch.load(os.path.join(save_dir, "best_caption_model.pt"), map_location=device)
model.load_state_dict(ckpt["model_state_dict"])
model.to(device)
model.eval()

# Take a batch from val loader
images, caps, lengths = next(iter(val_loader))
images = images.to(device)
hypo_ids = model.decode_greedy(images[:8], max_len=30, start_idx=vocab.word2idx[vocab.start_token], end_idx=vocab.word2idx[vocab.end_token])
for i, hid in enumerate(hypo_ids):
    print("Pred:", ids_to_sentence(hid, vocab))
    print("GT:", ids_to_sentence(caps[i].cpu().tolist(), vocab))

# === Cell 13: Simple beam search (optional, small beam) ===
import heapq

def beam_search(model, images, beam_width=3, max_len=30, start_idx=None, end_idx=None):
    # images: (batch, 3, H, W) - we'll handle batch=1 for simplicity
    model.eval()
    if start_idx is None:
        start_idx = vocab.word2idx[vocab.start_token]
    if end_idx is None:
        end_idx = vocab.word2idx[vocab.end_token]
    with torch.no_grad():
        memory = model.encode_image(images)  # (batch, src_seq, d)
        memory = memory.permute(1,0,2)  # (src_seq, batch, d)
        # support only batch=1 easily in this simple beam implementation
        assert images.size(0) == 1, "Beam search helper supports batch size 1"
        # each item in heap: (-score, [token_ids], hidden?)  note: we use logits scores
        heap = [ (0.0, [start_idx]) ]
        completed = []
        for _ in range(max_len-1):
            new_heap = []
            for score, seq in heap:
                if seq[-1] == end_idx:
                    completed.append((score, seq))
                    continue
                # expand
                seq_tensor = torch.tensor(seq, dtype=torch.long, device=images.device).unsqueeze(0)  # (1, seq)
                tgt_emb = model.token_emb(seq_tensor)
                tgt = tgt_emb.permute(1,0,2)
                tgt = model.pos_enc(tgt)
                tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(images.device)
                out = model.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)
                logits = model.generator(out[-1])  # (batch=1, vocab)
                log_probs = torch.log_softmax(logits, dim=-1).squeeze(0)
                topk = torch.topk(log_probs, beam_width)
                for k in range(beam_width):
                    next_tok = topk.indices[k].item()
                    next_score = score + topk.values[k].item()
                    new_seq = seq + [next_tok]
                    new_heap.append((next_score, new_seq))
            # keep top beam_width
            heap = heapq.nlargest(beam_width, new_heap, key=lambda x:x[0])
        completed.extend(heap)
        # choose best completed
        best = max(completed, key=lambda x:x[0])
        return best[1]

# === Cell 14: Save small model & vocab for deployment ===
torch.save({
    "model_state": model.state_dict(),
    "vocab": vocab.word2idx
}, "caption_model_deploy.pt")
print("Saved deploy checkpoint: caption_model_deploy.pt")

files.download("caption_model_deploy.pt")

import requests
from io import BytesIO
import torch
from PIL import Image
import matplotlib.pyplot as plt

# --------------------------
# Load Image (URL or Path)
# --------------------------
def load_image(img_input):
    if isinstance(img_input, str):
        if img_input.startswith("http"):
            resp = requests.get(img_input)
            resp.raise_for_status()
            return Image.open(BytesIO(resp.content)).convert("RGB")
        else:
            return Image.open(img_input).convert("RGB")
    elif isinstance(img_input, Image.Image):
        return img_input
    else:
        raise ValueError("Invalid image type")


# --------------------------
# Generate Caption
# --------------------------
def generate_caption(model, img_tensor, vocab, max_len=30):
    model.eval()
    with torch.no_grad():
        ids = model.decode_greedy(
            img_tensor,
            max_len,
            vocab.word2idx[vocab.start_token],
            vocab.word2idx[vocab.end_token]
        )
        return ids_to_sentence(ids[0], vocab)


# --------------------------
# Caption multiple images in rows (3 per row)
# --------------------------
def caption_images_side_by_side(image_list, model, vocab, transform, device, max_len=30, display_size=(250,250)):
    captions = []
    imgs = []

    # Preprocess + Caption
    for img_input in image_list:
        img = load_image(img_input)

        # Preprocess for model
        tensor = transform(img).unsqueeze(0).to(device)
        caption = generate_caption(model, tensor, vocab, max_len=max_len)

        # Resize for display
        small = img.copy()
        small.thumbnail(display_size)

        imgs.append(small)
        captions.append(caption)

    # -----------------------
    # Display images in rows of 3
    # -----------------------
    n = len(imgs)
    cols = 3
    rows = (n + cols - 1) // cols

    plt.figure(figsize=(5 * cols, 5 * rows))

    for i in range(n):
        plt.subplot(rows, cols, i + 1)
        plt.imshow(imgs[i])
        plt.axis("off")
        plt.title(captions[i], fontsize=10)

    plt.tight_layout()
    plt.show()

    return list(zip(imgs, captions))


# --------------------------
# Example Run
# --------------------------
images_to_test = [
    "/content/mini_coco/images/000000036238.jpg",
    "/content/mini_coco/images/000000000387.jpg",
    "/content/mini_coco/images/000000035887.jpg",
    "/content/mini_coco/images/000000039963.jpg",
    "/content/mini_coco/images/000000040635.jpg",
    "/content/mini_coco/images/000000044559.jpg",
]

caption_images_side_by_side(
    images_to_test,
    model,
    vocab,
    transform,
    device,
    max_len=30,
    display_size=(250,250)
)

import requests
from io import BytesIO
import torch
from PIL import Image, ImageFilter, ImageEnhance
import matplotlib.pyplot as plt


# ================================
# LOAD IMAGE
# ================================
def load_image(img_input, enhance=True):
    if isinstance(img_input, str):
        if img_input.startswith("http"):
            resp = requests.get(img_input)
            resp.raise_for_status()
            img = Image.open(BytesIO(resp.content)).convert("RGB")
        else:
            img = Image.open(img_input).convert("RGB")
    else:
        img = img_input

    # --- Enhance the image (improves captioning) ---
    if enhance:
        img = img.filter(ImageFilter.SHARPEN)
        img = ImageEnhance.Contrast(img).enhance(1.15)
        img = img.resize((512, 512), Image.LANCZOS)

    return img


# ================================
# CLEAN CAPTION (important!)
# ================================
def clean_caption(text):
    if not text:
        return ""

    # Remove spacing errors
    text = text.replace(" .", ".").replace(" ,", ",")
    text = text.replace("  ", " ").strip()

    # Remove repeating words
    words = text.split()
    new_words = [words[0]]
    for w in words[1:]:
        if w != new_words[-1]:
            new_words.append(w)

    text = " ".join(new_words)

    # Capitalize first letter
    text = text[0].upper() + text[1:]

    # Ensure it ends with a period
    if not text.endswith("."):
        text += "."

    return text


# ================================
# BEAM SEARCH (NO TRAINING NEEDED)
# ================================
def decode_with_beam_search(model, img_tensor, vocab, max_len=30, beam_size=5):
    model.eval()
    if img_tensor.shape[0] != 1:
        raise ValueError("Beam search helper currently supports batch size 1 only.")
    with torch.no_grad():
        # Call the standalone beam_search function
        ids = beam_search(
            model, # Pass the model object
            img_tensor, # Image tensor
            beam_width=beam_size,
            max_len=max_len,
            start_idx=vocab.word2idx[vocab.start_token],
            end_idx=vocab.word2idx[vocab.end_token]
        )
    return [ids] # beam_search returns a single list of IDs, wrap it in a list to match decode_greedy's output format


# ================================
# GENERATE CAPTION
# ================================
def generate_caption(model, img_tensor, vocab, max_len=30, use_beam=True):
    if use_beam:
        ids = decode_with_beam_search(model, img_tensor, vocab, max_len=max_len)
    else:
        # fallback: greedy
        ids = model.decode_greedy(
            img_tensor,
            max_len,
            vocab.word2idx[vocab.start_token],
            vocab.word2idx[vocab.end_token]
        )

    caption = ids_to_sentence(ids[0], vocab)
    caption = clean_caption(caption)
    return caption


# ================================
# DISPLAY IMAGES 3 PER ROW
# ================================
def caption_images_side_by_side(image_list, model, vocab, transform, device,
                                max_len=30, display_size=(250,250)):

    captions = []
    imgs = []

    for img_input in image_list:
        img = load_image(img_input)

        # preprocess for model
        tensor = transform(img).unsqueeze(0).to(device)

        # caption
        caption = generate_caption(model, tensor, vocab, max_len=max_len, use_beam=True)

        # resize for display
        small = img.copy()
        small.thumbnail(display_size)

        imgs.append(small)
        captions.append(caption)

    # ==== Plot 3 per row ====
    n = len(imgs)
    cols = 3
    rows = (n + cols - 1) // cols

    plt.figure(figsize=(5 * cols, 5 * rows))

    for i in range(n):
        plt.subplot(rows, cols, i + 1)
        plt.imshow(imgs[i])
        plt.axis("off")
        plt.title(captions[i], fontsize=10)

    plt.tight_layout()
    plt.show()

    return list(zip(imgs, captions))


# ================================
# EXAMPLE USAGE
# ================================
images_to_test = [
    "/content/1000_F_62677552_ozFYQC2IbhXavw3TVRzrUzkkUrhDL7bz.jpg",
    "/content/pexels-samrana3003-1442005.jpg",
    "/content/istockphoto-1368965646-612x612.jpg"
]

caption_images_side_by_side(
    images_to_test,
    model,
    vocab,
    transform,
    device,
    max_len=30,
    display_size=(250,250)
)

